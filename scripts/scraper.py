import os
import json
import requests
import re
import time
from bs4 import BeautifulSoup
from datetime import datetime

# ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥
API_KEY = os.environ.get('SCRAPER_API_KEY')

def load_journals():
    try:
        with open('data/journals.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âŒ Error: data/journals.json not found.")
        return []

def get_soup(target_url):
    if not API_KEY:
        print("âŒ ç¼ºå°‘ API Keyï¼")
        return None

    payload = {
        'api_key': API_KEY,
        'url': target_url,
        'render': 'true', 
    }
    
    try:
        # é‡è¯• 3 æ¬¡
        for attempt in range(3):
            r = requests.get('http://api.scraperapi.com', params=payload, timeout=60)
            if r.status_code == 200:
                return BeautifulSoup(r.text, 'html.parser')
            print(f"   âš ï¸ Attempt {attempt+1} failed: {r.status_code}. Retrying...")
            time.sleep(2)
        return None
    except Exception as e:
        print(f"   âŒ Network Error: {e}")
        return None

def extract_details(soup):
    """
    é€šç”¨çŠ¶æ€æœºè§£æå™¨ï¼šé€‚é… RSE å’Œ Cities ç­‰ä¸åŒæ’ç‰ˆç»“æ„
    """
    if not soup:
        return {"deadline": "Unknown", "editors": "Unknown", "description": ""}

    # --- 1. å…¨å±€å¤§æ¸…æ´— (ä¿æŒä¸å˜) ---
    for tag in soup(["header", "footer", "nav", "script", "style", "noscript", "iframe", ".banner", ".cookie-notice", ".submit-search-button-wrap"]):
        tag.decompose()

    full_text = soup.get_text(" ", strip=True)

    # --- 2. æå–æˆªæ­¢æ—¥æœŸ (æœ€ç¨³çš„é”šç‚¹) ---
    deadline = "Check Detail"
    deadline_node = None
    
    # å°è¯•å¯»æ‰¾åŒ…å« "Submission deadline" çš„èŠ‚ç‚¹
    # è¿™æ˜¯ä¸€ä¸ªå…³é”®é”šç‚¹ï¼Œæˆ‘ä»¬éšåä¼šä»è¿™ä¸ªä½ç½®å¼€å§‹å¾€ä¸‹ä¸€è¡Œä¸€è¡Œè¯»
    try:
        target_str = re.compile("Submission deadline", re.IGNORECASE)
        deadline_node = soup.find(string=target_str)
        
        if deadline_node:
            # è·å–æ—¥æœŸæ–‡æœ¬
            parent = deadline_node.parent
            # å¦‚æœæ˜¯åœ¨ strong æ ‡ç­¾é‡Œ
            strong = parent.find("strong")
            if strong:
                deadline = strong.get_text(strip=True)
            else:
                # å¦åˆ™å–å†’å·åé¢çš„æ–‡å­—
                deadline = parent.get_text(strip=True).split(":")[-1].strip()
            
            # å°†é”šç‚¹æå‡åˆ°å—çº§å…ƒç´  (div æˆ– p)ï¼Œä»¥ä¾¿æŸ¥æ‰¾å…„å¼ŸèŠ‚ç‚¹
            deadline_node = parent.find_parent(['div', 'p'])
    except:
        pass

    # --- 3. æµå¼æå– (State Machine) ---
    editors_parts = []
    description_parts = []
    
    # åˆå§‹çŠ¶æ€ï¼šé»˜è®¤ä¸º "description" (å› ä¸º Cities æŠŠç®€ä»‹æ”¾åœ¨æœ€å‰é¢)
    # çŠ¶æ€æšä¸¾: 'description', 'editors', 'stop'
    current_mode = 'description' 

    if deadline_node:
        # è·å– deadline ä¹‹åçš„æ‰€æœ‰åŒçº§å…ƒç´ 
        siblings = deadline_node.find_next_siblings()
        
        for tag in siblings:
            text = tag.get_text(strip=True)
            text_lower = text.lower()

            # --- A. çŠ¶æ€åˆ‡æ¢æ£€æŸ¥ ---
            
            # 1. é‡åˆ° "Guest editors" -> åˆ‡æ¢åˆ°ç¼–è¾‘æ¨¡å¼
            if "guest editors" in text_lower and len(text) < 50: # é•¿åº¦é™åˆ¶é˜²æ­¢è¯¯åˆ¤æ­£æ–‡
                current_mode = 'editors'
                continue # è·³è¿‡æ ‡é¢˜æœ¬èº«

            # 2. é‡åˆ° "Special issue information" -> åˆ‡æ¢å›ç®€ä»‹æ¨¡å¼
            if "special issue information" in text_lower and len(text) < 100:
                current_mode = 'description'
                continue

            # 3. é‡åˆ° "Manuscript submission" æˆ– "Keywords" -> åœæ­¢è§£æ
            if "manuscript submission" in text_lower or "keywords:" in text_lower:
                break

            # --- B. æ•°æ®æ”¶é›† ---
            
            if current_mode == 'editors':
                # è¿‡æ»¤ç©ºè¡Œ
                if len(text) > 2:
                    # Cities çš„ç¼–è¾‘åœ¨ div é‡Œï¼ŒRSE åœ¨ p é‡Œï¼Œè¿™é‡Œéƒ½å…¼å®¹
                    # ç®€å•æ¸…æ´—ï¼šç§»é™¤ "Email:" è¿™ç§å¹²æ‰°è¯
                    clean_editor = text.replace("Email:", "").strip()
                    editors_parts.append(clean_editor)

            elif current_mode == 'description':
                # åªæ”¶é›†æ®µè½å’Œåˆ—è¡¨ï¼Œå¿½ç•¥å¤ªçŸ­çš„åƒåœ¾å­—ç¬¦
                if tag.name in ['p', 'ul', 'ol', 'div'] and len(text) > 10:
                    # ç§»é™¤å†…è”æ ·å¼ï¼Œä¿ç•™ HTML ç»“æ„ (ä¸ºäº†æ¢è¡Œå’Œåˆ—è¡¨)
                    del tag['style']
                    del tag['class']
                    # ç§»é™¤å†…éƒ¨çš„é“¾æ¥ (é¿å…ç‚¹è¿›å»è·³å‡º)
                    for a in tag.find_all('a'):
                        a.unwrap() # ä¿ç•™æ–‡å­—ï¼Œç§»é™¤ <a> æ ‡ç­¾
                        
                    description_parts.append(str(tag))

    # --- 4. æ•°æ®ç»„è£… ---
    
    # ç¼–è¾‘ï¼šç”¨æ¢è¡Œç¬¦è¿æ¥
    editors_str = "<br>".join(editors_parts) if editors_parts else "Editors info not found."
    
    # ç®€ä»‹ï¼šæ‹¼æ¥ HTML
    description_html = "".join(description_parts)
    if not description_html:
        description_html = "Detailed description available on the official website."

    return {
        "deadline": deadline,
        "editors": editors_str, 
        "description": description_html
    }

def parse_journal(journal):
    print(f"ğŸ“– Scanning List: {journal['name']}...")
    soup = get_soup(journal['url'])
    issues = []
    
    if not soup: return []

    links = soup.select('a[href*="/special-issue/"]')
    print(f"   ğŸ” Found {len(links)} issues in list.")

    seen_urls = set()
    
    # âš ï¸ æ³¨æ„ï¼šä¸ºäº†æµ‹è¯•ï¼Œæˆ‘è¿™é‡Œè¿˜æ˜¯é™åˆ¶æŠ“å–å‰ 5 ä¸ª
    # å¦‚æœè¦å…¨æŠ“ï¼Œè¯·å»æ‰ [:5]
    for link in links[:5]: 
        title = link.get_text(strip=True)
        url = link.get('href')
        
        if not title or not url: continue
        if not url.startswith('http'): url = 'https://www.sciencedirect.com' + url
            
        if url not in seen_urls:
            seen_urls.add(url)
            print(f"      â˜ï¸ Deep diving: {title[:30]}...")
            
            # è¿›å…¥è¯¦æƒ…é¡µ
            detail_soup = get_soup(url)
            
            # æå–æ‰€æœ‰è¯¦æƒ…
            details = extract_details(detail_soup)
            
            print(f"      ğŸ—“ï¸ Deadline: {details['deadline']}")
            print(f"      ğŸ‘¥ Editors: {details['editors'][:30]}...")
            
            issues.append({
                'title': title,
                'url': url,
                'deadline': details['deadline'],
                'guest_editors': details['editors'],   # æ–°å¢å­—æ®µ
                'description': details['description'], # æ–°å¢å­—æ®µ
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            })
            
    return issues

def main():
    print("=" * 60)
    print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    journals = load_journals()
    if not journals: return

    results = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'journals': []
    }
    
    for journal in journals:
        issues = parse_journal(journal)
        results['journals'].append({
            'name': journal['name'],
            'url': journal['url'],
            'special_issues': issues
        })
    
    os.makedirs('data', exist_ok=True)
    with open('data/issues.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ’¾ Data saved to data/issues.json")
    print("=" * 60)

if __name__ == "__main__":
    main()