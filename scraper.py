#!/usr/bin/env python3
import json
import os
import asyncio
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
import re

class PlaywrightJournalScraper:
    def __init__(self):
        # ç§»é™¤äº†å®¹æ˜“è¢«å°é”çš„ sciencedirect ç›´æ¥é“¾æ¥ï¼Œæ”¹ç”¨ Elsevier ä¸“é—¨çš„åˆ—è¡¨é¡µ
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.journals.elsevier.com/remote-sensing-of-environment/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.journals.elsevier.com/cities/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        # åº”ç”¨ Stealth æ’ä»¶éšè— Playwright ç‰¹å¾
        await stealth_async(page)
        
        special_issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            
            # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
            await page.goto(journal_info['url'], wait_until='networkidle', timeout=60000)
            
            # æ¨¡æ‹Ÿäººç±»ç¼“æ…¢æ»šåŠ¨é¡µé¢ï¼Œè§¦å‘æ‡’åŠ è½½å†…å®¹
            for _ in range(3):
                await page.mouse.wheel(0, 800)
                await asyncio.sleep(1)

            # è·å–é¡µé¢å†…å®¹é•¿åº¦ï¼Œç”¨äºæ’æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            content_length = len(await page.content())
            print(f"   Page content length: {content_length} characters")

            if content_length < 2000:
                print(f"   âš  Warning: Content too short. Might be blocked by bot detection.")

            # æå–é€»è¾‘
            special_issues = await self.extract_logic(page)
            print(f"   âœ“ Found {len(special_issues)} issues")

        except Exception as e:
            print(f"   âœ— Error: {str(e)[:100]}")
        finally:
            await page.close()
        
        return special_issues

    async def extract_logic(self, page) -> List[Dict]:
        """é’ˆå¯¹ Elsevier æœŸåˆŠåˆ—è¡¨é¡µçš„ç»“æ„åŒ–æå–"""
        issues = []
        
        # ç­–ç•¥ 1: å¯»æ‰¾å…¸å‹çš„æ–‡ç« å¡ç‰‡ç»“æ„
        # Elsevier åˆ—è¡¨é¡µé€šå¸¸ä½¿ç”¨ <h3> æˆ– <a> å¸¦æœ‰ç‰¹å®šçš„ title
        selectors = [
            'a[data-testid="article-list-title-link"]', 
            'div.pod-listing-header a',
            'h3'
        ]
        
        found_elements = []
        for selector in selectors:
            elements = await page.query_selector_all(selector)
            if elements:
                found_elements = elements
                break

        for el in found_elements:
            try:
                title = await el.inner_text()
                href = await el.get_attribute('href')
                
                # è¿‡æ»¤æ— å…³é“¾æ¥
                title_lower = title.lower().strip()
                if len(title_lower) < 15 or any(x in title_lower for x in ['cookies', 'privacy', 'terms', 'guide for authors']):
                    continue

                # å¯»æ‰¾æ—¥æœŸ/æˆªæ­¢æ—¥æœŸ (åœ¨çˆ¶çº§æˆ–åç»­å…ƒç´ ä¸­)
                parent = await el.query_selector('xpath=..') # è·å–çˆ¶èŠ‚ç‚¹
                parent_text = await parent.inner_text() if parent else ""
                
                deadline = self.parse_deadline(parent_text)
                
                issues.append({
                    'title': title.strip(),
                    'url': href if href.startswith('http') else 'https://www.journals.elsevier.com' + href,
                    'deadline': deadline,
                    'last_seen': datetime.now().strftime('%Y-%m-%d')
                })
            except:
                continue
                
        return self.deduplicate(issues)

    def parse_deadline(self, text: str) -> str:
        # åŒ¹é…å¸¸ç”¨çš„æ—¥æœŸæ ¼å¼
        pattern = r'(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4})'
        match = re.search(pattern, text, re.IGNORECASE)
        return match.group(1) if match else "Not specified"

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            if i['title'].lower() not in seen:
                seen.add(i['title'].lower())
                unique.append(i)
        return unique

    async def run(self):
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # å…³é”®ï¼šä½¿ç”¨çœŸå®çš„ User-Agent
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
            )
            
            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'count': len(issues),
                    'special_issues': issues
                })
                await asyncio.sleep(5) # é¢‘ç‡æ§åˆ¶

            await browser.close()
            
        # å­˜å…¥æ–‡ä»¶
        os.makedirs('data', exist_ok=True)
        with open('data/special_issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… All done. Results saved to data/special_issues.json")

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())