import os
import json
import requests
import time
from bs4 import BeautifulSoup
from datetime import datetime

# ğŸ‘‡ å¼•å…¥æˆ‘ä»¬åˆšåˆšå†™çš„è§£æå™¨å·¥å‚
from parsers import get_parser

API_KEY = os.environ.get('SCRAPER_API_KEY')

def load_journals():
    try:
        with open('data/journals.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âŒ Error: journals.json not found.")
        return []

def get_soup(target_url):
    if not API_KEY:
        print("âŒ ç¼ºå°‘ API Keyï¼")
        return None
    # ... (è¿™é‡Œä¿æŒä½ åŸæ¥çš„ç½‘ç»œè¯·æ±‚ä»£ç ä¸å˜ï¼Œçœç•¥ä»¥èŠ‚çœç©ºé—´) ...
    # ... è®°å¾—æŠŠä¹‹å‰çš„ request é€»è¾‘æ”¾è¿™é‡Œ ...
    payload = { 'api_key': API_KEY, 'url': target_url, 'render': 'true' }
    try:
        for attempt in range(3):
            r = requests.get('http://api.scraperapi.com', params=payload, timeout=60)
            if r.status_code == 200: return BeautifulSoup(r.text, 'html.parser')
            time.sleep(2)
        return None
    except Exception as e:
        print(f"   âŒ Network Error: {e}")
        return None

def parse_journal(journal):
    print(f"ğŸ“– Scanning List: {journal['name']}...")
    soup = get_soup(journal['url'])
    issues = []
    if not soup: return []

    links = soup.select('a[href*="/special-issue/"]')
    print(f"   ğŸ” Found {len(links)} issues in list.")
    
    seen_urls = set()
    
    # æ‹¿åˆ°é’ˆå¯¹è¯¥æœŸåˆŠçš„è§£æå™¨å‡½æ•°ï¼
    specific_parser = get_parser(journal['name'])

    for link in links[:5]: 
        title = link.get_text(strip=True)
        url = link.get('href')
        if not title or not url: continue
        if not url.startswith('http'): url = 'https://www.sciencedirect.com' + url
            
        if url not in seen_urls:
            seen_urls.add(url)
            print(f"      â˜ï¸ Deep diving: {title[:30]}...")
            
            detail_soup = get_soup(url)
            
            # ğŸ‘‡ ç›´æ¥è°ƒç”¨é¢†åˆ°çš„è§£æå™¨ï¼Œä¸ç”¨ç®¡å†…éƒ¨å®ç°
            details = specific_parser(detail_soup)
            
            print(f"      ğŸ—“ï¸ Deadline: {details['deadline']}")
            
            issues.append({
                'title': title,
                'url': url,
                'deadline': details['deadline'],
                'guest_editors': details['editors'],
                'description': details['description'],
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            })
            
    return issues

def main():
    # ... (ä¿æŒä¸å˜) ...
    print("=" * 60)
    print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    journals = load_journals()
    if not journals: return

    results = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'journals': []
    }
    
    for journal in journals:
        issues = parse_journal(journal)
        journal_data = journal.copy()
        journal_data['special_issues'] = issues
        
        results['journals'].append(journal_data)
    
    os.makedirs('data', exist_ok=True)
    with open('data/issues.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ’¾ Data saved to data/issues.json")
    print("=" * 60)

if __name__ == "__main__":
    main()