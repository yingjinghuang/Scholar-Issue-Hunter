#!/usr/bin/env python3
import json
import os
import asyncio
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
import re

class PlaywrightJournalScraper:
    def __init__(self):
        # ç§»é™¤äº†å®¹æ˜“è¢«å°é”çš„ sciencedirect ç›´æ¥é“¾æ¥ï¼Œæ”¹ç”¨ Elsevier ä¸“é—¨çš„åˆ—è¡¨é¡µ
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.journals.elsevier.com/remote-sensing-of-environment/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.journals.elsevier.com/cities/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        # åº”ç”¨ Stealth æ’ä»¶éšè— Playwright ç‰¹å¾
        await stealth_async(page)
        
        special_issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            
            # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
            await page.goto(journal_info['url'], wait_until='networkidle', timeout=60000)
            
            # æ¨¡æ‹Ÿäººç±»ç¼“æ…¢æ»šåŠ¨é¡µé¢ï¼Œè§¦å‘æ‡’åŠ è½½å†…å®¹
            for _ in range(3):
                await page.mouse.wheel(0, 800)
                await asyncio.sleep(1)

            # è·å–é¡µé¢å†…å®¹é•¿åº¦ï¼Œç”¨äºæ’æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            content_length = len(await page.content())
            print(f"   Page content length: {content_length} characters")

            if content_length < 2000:
                print(f"   âš  Warning: Content too short. Might be blocked by bot detection.")

            # æå–é€»è¾‘
            special_issues = await self.extract_logic(page)
            print(f"   âœ“ Found {len(special_issues)} issues")

        except Exception as e:
            print(f"   âœ— Error: {str(e)[:100]}")
        finally:
            await page.close()
        
        return special_issues

    async def extract_logic(self, page) -> List[Dict]:
        """ä¸“é—¨é’ˆå¯¹ ScienceDirect/Elsevier é¡µé¢ç»“æ„çš„æå–é€»è¾‘"""
        issues = []
        
        # 1. ç­‰å¾…åˆ—è¡¨åŠ è½½
        try:
            await page.wait_for_selector('li.list-item', timeout=10000)
        except:
            print("   âš  Timeout waiting for 'li.list-item', checking full body...")

        # 2. å®šä½æ‰€æœ‰çš„åˆ—è¡¨é¡¹
        items = await page.query_selector_all('li.list-item')
        
        for item in items:
            try:
                # æå–æ ‡é¢˜å’Œ URL
                # æºç å¯¹åº”ï¼š<h3><a class="anchor title ..." href="...">
                title_elem = await item.query_selector('h3 a.title')
                if not title_elem:
                    continue
                
                title = await title_elem.inner_text()
                url = await title_elem.get_attribute('href')
                
                # æå–æˆªæ­¢æ—¥æœŸ
                # æºç å¯¹åº”ï¼š<div class="text-xs ...">Submission deadline: <strong>30 June 2026</strong></div>
                deadline = "Not specified"
                deadline_elem = await item.query_selector('div.text-xs')
                if deadline_elem:
                    deadline_text = await deadline_elem.inner_text()
                    # ä½¿ç”¨æ­£åˆ™æå– Submission deadline ä¹‹åçš„å†…å®¹
                    match = re.search(r'deadline:\s*(.*)', deadline_text, re.IGNORECASE)
                    if match:
                        deadline = match.group(1).strip()

                # æå–å®¢åº§ç¼–è¾‘ (Guest Editors)
                # æºç å¯¹åº”ï¼š<p class="summary ...">Guest editors: Le Wang, ...</p>
                editors = None
                editor_elem = await item.query_selector('p.summary')
                if editor_elem:
                    editor_text = await editor_elem.inner_text()
                    editors = editor_text.replace('Guest editors:', '').strip()

                issues.append({
                    'title': title.strip(),
                    'url': url if url.startswith('http') else 'https://www.sciencedirect.com' + url,
                    'deadline': deadline,
                    'guest_editors': editors,
                    'last_seen': datetime.now().strftime('%Y-%m-%d')
                })
                
            except Exception as e:
                print(f"   âš  Error parsing an item: {str(e)[:50]}")
                continue
                
        return self.deduplicate(issues)

    def parse_deadline(self, text: str) -> str:
        # åŒ¹é…å¸¸ç”¨çš„æ—¥æœŸæ ¼å¼
        pattern = r'(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4})'
        match = re.search(pattern, text, re.IGNORECASE)
        return match.group(1) if match else "Not specified"

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            if i['title'].lower() not in seen:
                seen.add(i['title'].lower())
                unique.append(i)
        return unique

    async def run(self):
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # å…³é”®ï¼šä½¿ç”¨çœŸå®çš„ User-Agent
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
            )
            
            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'count': len(issues),
                    'special_issues': issues
                })
                await asyncio.sleep(5) # é¢‘ç‡æ§åˆ¶

            await browser.close()
            
        # å­˜å…¥æ–‡ä»¶
        os.makedirs('data', exist_ok=True)
        with open('data/special_issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… All done. Results saved to data/special_issues.json")

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())