#!/usr/bin/env python3
import json
import os
import asyncio
import re
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright

# å…¼å®¹æ€§å¤„ç†ï¼šé€‚é…ä¸åŒç‰ˆæœ¬çš„ playwright-stealth
try:
    from playwright_stealth import stealth_async
except ImportError:
    async def stealth_async(page):
        import playwright_stealth
        await playwright_stealth.stealth_async(page)

class PlaywrightJournalScraper:
    def __init__(self):
        # ä½¿ç”¨ä½ æä¾›æºç çš„ ScienceDirect ç›®æ ‡é¡µé¢
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        await stealth_async(page)
        
        issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            # 1. è®¿é—®é¡µé¢
            await page.goto(journal_info['url'], wait_until='domcontentloaded', timeout=60000)
            
            # 2. å…³é”®ï¼šç­‰å¾…æ•°æ®åˆ—è¡¨å®¹å™¨æ¸²æŸ“å®Œæˆ (åŸºäºä½ æä¾›çš„æºç ç±»å)
            try:
                await page.wait_for_selector('li.list-item', timeout=20000)
                # é¢å¤–ç¼“å†²ï¼Œç¡®ä¿ React åˆ—è¡¨æ¸²æŸ“å®Œæ•´
                await asyncio.sleep(2) 
            except:
                print(f"   âš  Timeout: 'li.list-item' not found. Page might be empty or loading too slow.")

            # 3. æ»šåŠ¨ä»¥è§¦å‘å¯èƒ½çš„æ‡’åŠ è½½
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(1)

            # 4. æ‰§è¡ŒæŠ“å–é€»è¾‘
            issues = await self.extract_logic(page)
            print(f"   âœ“ Success: Found {len(issues)} special issues")

        except Exception as e:
            print(f"   âœ— Error scraping {journal_info['name']}: {str(e)[:100]}")
        finally:
            await page.close()
        
        return issues

    async def extract_logic(self, page) -> List[Dict]:
        """é’ˆå¯¹ ScienceDirect HTML ç»“æ„çš„ç²¾å‡†æå–"""
        scraped_data = []
        
        # å®šä½æ‰€æœ‰çš„åˆ—è¡¨æ¡ç›®
        items = await page.query_selector_all('li.list-item')
        
        for item in items:
            try:
                # A. æå–æ ‡é¢˜å’Œ URL (åŸºäºæºç : a.anchor.title)
                title_link = await item.query_selector('h3 a.anchor.title')
                if not title_link:
                    continue
                
                title = await title_link.inner_text()
                href = await title_link.get_attribute('href')
                
                # B. æå–æˆªæ­¢æ—¥æœŸ (åŸºäºæºç : div.text-xs)
                deadline = "Not specified"
                deadline_elem = await item.query_selector('div.text-xs')
                if deadline_elem:
                    deadline_text = await deadline_elem.inner_text()
                    # æ­£åˆ™åŒ¹é…æ—¥æœŸéƒ¨åˆ†
                    match = re.search(r'deadline:\s*(.*)', deadline_text, re.IGNORECASE)
                    if match:
                        deadline = match.group(1).strip()

                # C. æå–å®¢åº§ç¼–è¾‘ (åŸºäºæºç : p.summary)
                editors = "Not specified"
                editor_elem = await item.query_selector('p.summary')
                if editor_elem:
                    editor_text = await editor_elem.inner_text()
                    editors = editor_text.replace('Guest editors:', '').strip()

                # è¡¥å…¨ URL
                full_url = href if href.startswith('http') else 'https://www.sciencedirect.com' + href

                scraped_data.append({
                    'title': title.strip(),
                    'url': full_url,
                    'deadline': deadline,
                    'guest_editors': editors,
                    'last_updated': datetime.now().strftime('%Y-%m-%d')
                })
            except:
                continue
                
        # å¦‚æœ li æŠ“å–å¤±è´¥ï¼Œå¯åŠ¨æ–¹æ¡ˆ Bï¼šç›´æ¥æŠ“å–æ‰€æœ‰ SI é“¾æ¥
        if not scraped_data:
            print("   ğŸ” Falling back to Link-based scan...")
            all_si_links = await page.query_selector_all('a[href*="/special-issue/"]')
            for link in all_si_links:
                try:
                    t = await link.inner_text()
                    u = await link.get_attribute('href')
                    if len(t) > 15:
                        scraped_data.append({
                            'title': t.strip(),
                            'url': u if u.startswith('http') else 'https://www.sciencedirect.com' + u,
                            'deadline': "See link",
                            'guest_editors': "See link",
                            'last_updated': datetime.now().strftime('%Y-%m-%d')
                        })
                except: continue

        return self.deduplicate(scraped_data)

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            key = i['title'].lower().strip()
            if key not in seen:
                seen.add(key)
                unique.append(i)
        return unique

    async def run(self):
        print("=" * 60)
        print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # å¿…é¡»ä½¿ç”¨ chromium å¹¶åœ¨ headless æ¨¡å¼ä¸‹é…ç½®çœŸå®çš„ä¸Šä¸‹æ–‡
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'url': journal['url'],
                    'special_issues': issues
                })
                # ç¤¼è²Œæ€§å»¶è¿Ÿï¼Œé˜²æ­¢ IP è§¦å‘äºŒæ¬¡æ‹¦æˆª
                await asyncio.sleep(5)

            await browser.close()
            
        # ä¿å­˜ç»“æœ
        os.makedirs('data', exist_ok=True)
        with open('data/special_issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Scraping completed. Data saved to data/special_issues.json")
        print("=" * 60)

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())